{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executed query:\n",
    "```sparql\n",
    "SELECT ?item ?itemLabel ?itemDescription ?subject ?subjectLabel WHERE {\n",
    "  VALUES ?field { wd:Q2267705 wd:Q11862829 }\n",
    "  \n",
    "  ?subject (wdt:P31|wdt:P279) ?field.\n",
    "  ?item (wdt:P361|wdt:P2579) ?subject.\n",
    "  ?subject rdfs:label ?subjectLabel.\n",
    "  FILTER (LANG(?subjectLabel) = \"en\" && !REGEX(?subjectLabel, \"^Q[0-9]+$\"))\n",
    "  \n",
    "  OPTIONAL { ?item schema:description ?itemDescription. FILTER(LANG(?itemDescription) = \"en\") }\n",
    "  \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "query = '''\n",
    "SELECT ?item ?itemLabel ?itemDescription (GROUP_CONCAT(DISTINCT ?subjectLabel; SEPARATOR=\", \") AS ?subjectLabels) (GROUP_CONCAT(DISTINCT ?subitemLabel; SEPARATOR=\", \") AS ?subitemLabels) WHERE {\n",
    "  VALUES ?subject { wd:Q131476 wd:Q21198 wd:Q11205 wd:Q4809258}  # Graph theory, Computer science, Arithmetics\n",
    "  \n",
    "  #?subject (wdt:P31|wdt:P279) ?field.\n",
    "  ?item (wdt:P361|wdt:P2579|wdt:P279) ?subject.\n",
    "  \n",
    "  ?item schema:description ?itemDescription.\n",
    "  FILTER(LANG(?itemDescription) = \"en\")\n",
    "  \n",
    "  ?subject rdfs:label ?subjectLabel.\n",
    "  FILTER (LANG(?subjectLabel) = \"en\" && !REGEX(?subjectLabel, \"^Q[0-9]+$\"))\n",
    "  \n",
    "  OPTIONAL { ?subitem wdt:P279 ?item.\n",
    "             ?subitem rdfs:label ?subitemLabel.\n",
    "             FILTER(LANG(?subitemLabel) = \"en\") }\n",
    "  \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "} GROUP BY ?item ?itemLabel ?itemDescription\n",
    "'''\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query})\n",
    "print(r)\n",
    "data = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining a list of all academic disciplines with their parent disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "disciplines_query = '''SELECT ?discipline ?disciplineLabel ?disciplineDescription \n",
    "       (GROUP_CONCAT(DISTINCT ?parentLabel; separator=\";\") AS ?parentDisciplines) \n",
    "WHERE {\n",
    "  ?discipline wdt:P31 wd:Q11862829.\n",
    "  ?discipline rdfs:label ?disciplineLabel.\n",
    "  \n",
    "  # Ensure the discipline has at least one \"studied by\" (P2579) incoming relationship\n",
    "  FILTER EXISTS { ?concept (wdt:P366|wdt:P31|wdt:P2579) ?discipline. }\n",
    "  \n",
    "  # Get discipline description\n",
    "  OPTIONAL {\n",
    "    ?discipline schema:description ?disciplineDescription.\n",
    "    FILTER(LANG(?disciplineDescription) = \"en\")\n",
    "  }\n",
    "  \n",
    "  # Find parent disciplines (P361 - \"part of\", P1269 - \"facet of\")\n",
    "  OPTIONAL {\n",
    "    ?discipline (wdt:P361|wdt:P1269) ?parentDiscipline.\n",
    "    ?parentDiscipline wdt:P31 wd:Q11862829.  # Ensure parent is also an academic discipline\n",
    "    ?parentDiscipline rdfs:label ?parentLabel.\n",
    "    FILTER(LANG(?parentLabel) = \"en\")\n",
    "  }\n",
    "  \n",
    "  # Ensure the discipline label is in English\n",
    "  FILTER(LANG(?disciplineLabel) = \"en\")\n",
    "  FILTER(!REGEX(STR(?disciplineLabel), \"^Q[0-9]+$\")) # Exclude labels that look like QIDs\n",
    "} \n",
    "GROUP BY ?discipline ?disciplineLabel ?disciplineDescription\n",
    "ORDER BY ?disciplineLabel\n",
    "'''\n",
    "r = requests.get(url, params = {'format': 'json', 'query': disciplines_query})\n",
    "data = r.json()\n",
    "df = pd.DataFrame(data['results']['bindings'])\n",
    "for column_name in df.columns:\n",
    "\tdf[column_name] = df[column_name].apply(lambda x: x['value'] if isinstance(x, dict) else '')\n",
    "df.to_csv(\"wikidata_disciplines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "df = pd.read_csv(\"wikidata_disciplines.csv\")\n",
    "\n",
    "WIKIDATA_SPARQL_URL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def fetch_related_concepts(discipline_uri):\n",
    "    query = f\"\"\"\n",
    "    SELECT (GROUP_CONCAT(DISTINCT ?conceptLabel; separator=\";\") AS ?relatedConcepts) WHERE {{\n",
    "      ?concept (wdt:P361|wdt:P2579|wdt:P279|wdt:P31) <{discipline_uri}>.\n",
    "      ?concept rdfs:label ?conceptLabel.\n",
    "      FILTER(LANG(?conceptLabel) = \"en\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = requests.get(WIKIDATA_SPARQL_URL, params={\"query\": query, \"format\": \"json\"},)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"results\" in data and \"bindings\" in data[\"results\"]:\n",
    "            bindings = data[\"results\"][\"bindings\"]\n",
    "            if bindings:\n",
    "                return bindings[0].get(\"relatedConcepts\", {}).get(\"value\", \"\")\n",
    "    return \"\"\n",
    "\n",
    "df[\"relatedConcepts\"] = \"\"\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    discipline_uri = row[\"discipline\"]\n",
    "    related_concepts = fetch_related_concepts(discipline_uri)\n",
    "    df.at[idx, \"relatedConcepts\"] = related_concepts\n",
    "    print(f\"Processed {row['disciplineLabel']} - Related Concepts: {related_concepts}\")\n",
    "    \n",
    "    # Respect Wikidata's rate limits\n",
    "    time.sleep(1)  # 1-second delay to avoid excessive requests\n",
    "\n",
    "# Save results to a new CSV file\n",
    "# df.to_csv(\"wikidata_disciplines_with_related_concepts.csv\", index=False)\n",
    "print(\"Saved related concepts to 'wikidata_disciplines_with_related_concepts.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"wikidata_disciplines_with_related_concepts.csv\")\n",
    "\n",
    "# Load the embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to create a meaningful text representation\n",
    "def create_text_representation(row):\n",
    "    if pd.notna(row[\"disciplineDescription\"]):  # Check if description exists\n",
    "        return f\"{row['disciplineLabel']}: {row['disciplineDescription']}\"\n",
    "    return row[\"disciplineLabel\"]\n",
    "\n",
    "# Generate text representations\n",
    "df[\"text_representation\"] = df.apply(create_text_representation, axis=1)\n",
    "\n",
    "# Compute embeddings using both name and description\n",
    "df[\"embedding\"] = df[\"text_representation\"].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Convert list of embeddings to a NumPy array\n",
    "embeddings_matrix = np.vstack(df[\"embedding\"].values)\n",
    "\n",
    "# Save embeddings and corresponding discipline data\n",
    "np.save(\"discipline_embeddings.npy\", embeddings_matrix)  # Save embeddings\n",
    "# df[[\"discipline\", \"disciplineLabel\", \"disciplineDescription\"]].to_csv(\"disciplines_metadata.csv\", index=False)  # Save metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                discipline  \\\n",
      "332          332   http://www.wikidata.org/entity/Q2555318   \n",
      "696          696   http://www.wikidata.org/entity/Q1003009   \n",
      "540          540    http://www.wikidata.org/entity/Q387196   \n",
      "475          475     http://www.wikidata.org/entity/Q21198   \n",
      "695          695   http://www.wikidata.org/entity/Q3984091   \n",
      "476          476  http://www.wikidata.org/entity/Q11492827   \n",
      "1438        1438   http://www.wikidata.org/entity/Q2878974   \n",
      "471          471    http://www.wikidata.org/entity/Q428691   \n",
      "1440        1440    http://www.wikidata.org/entity/Q844718   \n",
      "474          474     http://www.wikidata.org/entity/Q80006   \n",
      "\n",
      "                       disciplineLabel  \\\n",
      "332                automated reasoning   \n",
      "696                       formal logic   \n",
      "540                  description logic   \n",
      "475                   computer science   \n",
      "695             formal language theory   \n",
      "476   computer science and engineering   \n",
      "1438      theoretical computer science   \n",
      "471               computer engineering   \n",
      "1440             theory of computation   \n",
      "474               computer programming   \n",
      "\n",
      "                                  disciplineDescription  \\\n",
      "332              subfield of computer science and logic   \n",
      "696   study of propositions, statements, and deducti...   \n",
      "540   family of formal knowledge representation lang...   \n",
      "475                                study of computation   \n",
      "695   field of mathematical logic, theoretical lingu...   \n",
      "476   academic program at many universities which co...   \n",
      "1438       subfield of computer science and mathematics   \n",
      "471   engineering discipline specializing in the des...   \n",
      "1440                       subfield of computer science   \n",
      "474   the process of designing and building an execu...   \n",
      "\n",
      "                          parentDisciplines  similarity  \n",
      "332            theoretical computer science    0.651257  \n",
      "696                                     NaN    0.560353  \n",
      "540                                     NaN    0.543585  \n",
      "475                                     NaN    0.527310  \n",
      "695            theoretical computer science    0.477014  \n",
      "476                                     NaN    0.449305  \n",
      "1438                       computer science    0.437670  \n",
      "471                  electrical engineering    0.433021  \n",
      "1440   computer science;information science    0.427470  \n",
      "474   computer science;software development    0.422535  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_meta = pd.read_csv(\"wikidata_disciplines.csv\")  # Discipline metadata\n",
    "embeddings_matrix = np.load(\"discipline_embeddings.npy\")  # Precomputed embeddings\n",
    "\n",
    "def find_relevant_disciplines(course_name, top_n=10):\n",
    "    course_embedding = model.encode(course_name).reshape(1, -1)  # Compute course embedding\n",
    "    similarities = cosine_similarity(course_embedding, embeddings_matrix)[0]\n",
    "    df_meta[\"similarity\"] = similarities\n",
    "    top_disciplines = df_meta.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "    \n",
    "    return top_disciplines\n",
    "\n",
    "# Example:\n",
    "relevant_disciplines = find_relevant_disciplines(\"Logic in Computer Science: This is a lecture on foundations of Logic for Computer Science. Introduction to Logic and symbolic Knowledge Representation and Reasoning. Propositional Calculus: syntax, semantics, logical implication, inference, theorem proving. Design and analysis of logical models. Normal forms (CNF, DNF). The SAT problem. First Order Predicate Calculus: syntax, semantics, logical implication, inference, theorem proving. Design and analysis of logical models in FOPC. Normal forms (CNF, DNF). Resolution and Dual Resolution theorem proving. Introduction to Logic Programming and Constraint Programming.\")\n",
    "print(relevant_disciplines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class WikidataEntity:\n",
    "    def __init__(self, qid: str, label: str, description: str = \"\"):\n",
    "        self.qid = qid\n",
    "        self.label = label\n",
    "        self.description = description\n",
    "        self.url = f\"https://www.wikidata.org/wiki/{qid}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Qid: {self.qid}, label: {self.label}, description: {self.description}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"WikidataEntity('{self.qid}', '{self.label}', '{self.description}')\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\"qid\": self.qid, \"label\": self.label, \"description\": self.description}\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(data):\n",
    "        return WikidataEntity(data[\"qid\"], data[\"label\"], data.get(\"description\", \"\"))\n",
    "\n",
    "\n",
    "class WikidataMatcher:\n",
    "    def __init__(\n",
    "        self, similarity_threshold=0.3, cache_file: str = \"wikidata_cache.json\"\n",
    "    ):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self.load_cache()\n",
    "        self.max_nr_of_trials = 3\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    def load_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            return {k: WikidataEntity.from_dict(v) for k, v in data.items()}\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        with open(self.cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                {k: v.to_dict() for k, v in self.cache.items()},\n",
    "                f,\n",
    "                indent=2,\n",
    "                ensure_ascii=False,\n",
    "            )\n",
    "\n",
    "    def search_entity(self, query: str, course_name: str) -> Optional[WikidataEntity]:\n",
    "        # Check local cache first\n",
    "        if query in self.cache:\n",
    "            return self.cache[query]\n",
    "\n",
    "        # If not found, query Wikidata API\n",
    "        entities = self._search_wikidata(query, course_name)\n",
    "\n",
    "        if entities:\n",
    "            entity = entities[0]  # Take the best matching entity\n",
    "            if entity:\n",
    "                self.cache[query] = entity\n",
    "            self.save_cache()\n",
    "            return entity\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _search_wikidata(self, query: str, course_name:str) -> List[WikidataEntity]:\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": query,\n",
    "            \"format\": \"json\",\n",
    "            \"srlimit\": 5,\n",
    "            \"srnamespace\": \"0\",\n",
    "        }\n",
    "        trial_nr = 1\n",
    "        while trial_nr <= self.max_nr_of_trials:\n",
    "            response = requests.get(search_url, params=params)\n",
    "            if response.status_code != 200:\n",
    "                trial_nr += 1\n",
    "                print(\"sleeping...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "                # response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            if \"query\" not in data or \"search\" not in data[\"query\"]:\n",
    "                return []\n",
    "\n",
    "            titles = [item[\"title\"] for item in data[\"query\"][\"search\"]]\n",
    "            entities = self._get_entities_details(titles)\n",
    "\n",
    "            if not entities:\n",
    "                return []\n",
    "            \n",
    "            filtered_entities = self._filter_entities(entities)\n",
    "\n",
    "            if not filtered_entities:\n",
    "                print(f\"No valid Wikidata entities after filtering for query '{query}'.\")\n",
    "                return []\n",
    "\n",
    "            return [self._get_best_match(query, course_name, filtered_entities)]\n",
    "\n",
    "    def _get_best_match(\n",
    "        self, query: str, course_name:str, candidates: List[WikidataEntity]\n",
    "    ) -> WikidataEntity | None:\n",
    "        query_embedding = self.embedding_model.encode(f\"{query}\")\n",
    "        course_embedding = self.embedding_model.encode(f\"{course_name}\")\n",
    "\n",
    "        scored = []\n",
    "        for entity in candidates:\n",
    "            text = (\n",
    "                f\"{entity.label}: {entity.description}\"\n",
    "                if entity.description\n",
    "                else entity.label\n",
    "            )\n",
    "\n",
    "            candidate_embedding = self.embedding_model.encode(text)\n",
    "            query_similarity = cosine_similarity([query_embedding], [candidate_embedding])[0][\n",
    "                0\n",
    "            ]\n",
    "            course_similarity = cosine_similarity([course_embedding], [candidate_embedding])[0][\n",
    "                0\n",
    "            ]\n",
    "            if query_similarity >= self.similarity_threshold or course_similarity >= self.similarity_threshold:\n",
    "                scored.append((entity, query_similarity, course_similarity))\n",
    "        print(scored)\n",
    "        # Sort descending by similarity\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        if len(scored) == 0: return None\n",
    "\n",
    "        best_match, best_query_score, best_course_score = scored[0]\n",
    "        print(\n",
    "            f\"Best match for '{query}' = '{best_match.label}' (score: {best_query_score:.2f})\"\n",
    "        )\n",
    "\n",
    "        return best_match\n",
    "\n",
    "    def _get_entities_details(self, qids: List[str]) -> List[WikidataEntity]:\n",
    "        if not qids:\n",
    "            return []\n",
    "\n",
    "        sparql_url = \"https://query.wikidata.org/sparql\"\n",
    "        ids_formatted = \" \".join(f\"wd:{qid}\" for qid in qids)\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT ?item ?itemLabel ?itemDescription\n",
    "        WHERE {{\n",
    "            VALUES ?item {{ {ids_formatted} }} \n",
    "            FILTER NOT EXISTS {{\n",
    "                ?item wdt:P31 ?type .\n",
    "                FILTER (?type IN (wd:Q7725634, wd:Q18918145, wd:Q13442814, wd:Q1368848))\n",
    "            }}\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        trial_nr = 1\n",
    "        while trial_nr <= self.max_nr_of_trials:\n",
    "            r = requests.get(sparql_url, params={\"format\": \"json\", \"query\": query})\n",
    "            if r.status_code != 200:\n",
    "                trial_nr += 1\n",
    "                print(\"sleeping sparql ...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            data = r.json()\n",
    "            entities = {}\n",
    "\n",
    "            for item in data[\"results\"][\"bindings\"]:\n",
    "                label = item.get(\"itemLabel\", {}).get(\"value\", \"\")\n",
    "                description = item.get(\"itemDescription\", {}).get(\"value\", \"\")\n",
    "                qid = (\n",
    "                    item.get(\"item\", {})\n",
    "                    .get(\"value\")\n",
    "                    .replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                )\n",
    "                entities[qid] = WikidataEntity(qid, label, description)\n",
    "\n",
    "            return [entities[qid] for qid in qids if qid in entities]\n",
    "    \n",
    "    def _filter_entities(self, entities: List[WikidataEntity]) -> List[WikidataEntity]:\n",
    "        filtered = []\n",
    "        for e in entities:\n",
    "            label = e.label or \"\"\n",
    "            description = (e.description or \"\").lower()\n",
    "            if label == \"\" or description == \"\": continue\n",
    "\n",
    "            if \"category:\" in label.lower():\n",
    "                continue\n",
    "            if len(label.split()) > 4:\n",
    "                continue\n",
    "            if \"scientific article\" in description or \"scholarly article\" in description:\n",
    "                continue\n",
    "\n",
    "            filtered.append(e)\n",
    "        return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(WikidataEntity('Q1967302', 'Membership function', 'Membership function (mathematics)'), np.float32(0.49406463), np.float32(0.25334942)), (WikidataEntity('Q11348', 'function', 'association of a single output to each input'), np.float32(0.3674376), np.float32(0.17672753)), (WikidataEntity('Q7754', 'mathematical analysis', 'branch of mathematics'), np.float32(0.43703473), np.float32(0.7466788))]\n",
      "Best match for 'function (mathematics)' = 'Membership function' (score: 0.49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WikidataEntity('Q1967302', 'Membership function', 'Membership function (mathematics)')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata_matcher = WikidataMatcher()\n",
    "wikidata_matcher.search_entity('function (mathematics)', 'Mathematical analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"scholarly article\", \"scientific article\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
